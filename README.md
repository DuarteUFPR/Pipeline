#Desafio - Pipeline de dados

Este repositÃ³rio contÃ©m um pipeline completo de ETL (Extract, Transform, Load) projetado para reprodutibilidade mÃ¡xima. A execuÃ§Ã£o Ã© feita atravÃ©s de um comando Ãºnico (`make run`) em um ambiente de terminal (headless).

## âš™ï¸ Setup e PrÃ©-requisitos

Para executar o pipeline, vocÃª precisa ter instalados o **Python 3** e o **`make`**. O `make run` instala automaticamente todas as dependÃªncias Python necessÃ¡rias.

## ğŸš€ Como Rodar o Projeto

### Passo 1: PreparaÃ§Ã£o dos Dados

1.  Coloque seu arquivo de dados brutos CSV dentro da pasta `dados/`.
2.  O programa possibilita a navagaÃ§Ã£o direto para o arquivo CSV.

### Passo 2: ExecuÃ§Ã£o do ETL Completo

Execute o comando a seguir na pasta raiz do projeto:

```bash
make run
```

Este comando executa a sequÃªncia completa: InstalaÃ§Ã£o â†’ Bronze â†’ Silver â†’ Gold â†’ MÃ©tricas â†’ Testes Automatizados.

## ğŸ“Š Artefatos e Resultados

Os resultados da execuÃ§Ã£o sÃ£o salvos na pasta **`results/`**:
* **`metricas.json`**: Dados de tempo e contagem de linhas (Q3).
* **`throughput_tempo.png`**: GrÃ¡fico da performance por etapa.
* **`dedup_effect.png`**: GrÃ¡fico que mostra a reduÃ§Ã£o de linhas (deduplicaÃ§Ã£o).

## ğŸ—‘ï¸ Limpeza do Projeto

Para remover o banco de dados DuckDB e os arquivos de cache:

```bash
make clean
```
# Pipeline
# Pipeline
# Pipeline
